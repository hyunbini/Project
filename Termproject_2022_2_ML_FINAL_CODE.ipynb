{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9df8b830",
      "metadata": {
        "id": "9df8b830"
      },
      "source": [
        "Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "533ac460",
      "metadata": {
        "id": "533ac460"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff77a638",
      "metadata": {
        "id": "ff77a638"
      },
      "source": [
        "Load training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c16f4ca",
      "metadata": {
        "id": "2c16f4ca"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ML Data/training_dt/ratings_small.csv')\n",
        "movies_metadata = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ML Data/training_dt/movies_metadata.csv', low_memory = False)\n",
        "credits = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ML Data/training_dt/credits.csv')\n",
        "keywords = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ML Data/training_dt/keywords.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89ae0c44",
      "metadata": {
        "id": "89ae0c44"
      },
      "source": [
        "Load test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8616f41b",
      "metadata": {
        "id": "8616f41b"
      },
      "outputs": [],
      "source": [
        "#load test dataset\n",
        "test = np.load('/content/drive/MyDrive/Colab Notebooks/ML Data/test_dt/Dataset.npy')\n",
        "test = pd.DataFrame(test)\n",
        "test = test[0].str.split(\",\", expand=True).rename(columns={0:'userId', 1:\"imdb_id\", 2:\"rating\", 3:'rating_date'})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e9fa439",
      "metadata": {
        "id": "1e9fa439"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4b4ac96",
      "metadata": {
        "id": "e4b4ac96"
      },
      "source": [
        "### Training dataset preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a862c06",
      "metadata": {
        "id": "1a862c06"
      },
      "source": [
        "Rating preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "495de9da",
      "metadata": {
        "id": "495de9da"
      },
      "outputs": [],
      "source": [
        "ratings.rename(columns={'movieId':'id'}, inplace = True)\n",
        "ratings.loc['id'] = ratings['id'].astype('str')\n",
        "ratings['id'] = pd.to_numeric(ratings['id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c86f5867",
      "metadata": {
        "id": "c86f5867"
      },
      "source": [
        "Movies_metadata preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10504bb3",
      "metadata": {
        "id": "10504bb3"
      },
      "outputs": [],
      "source": [
        "#drop unnecessary features\n",
        "movies_metadata.drop(['homepage', 'tagline', 'poster_path'], axis = 1, inplace = True)\n",
        "movies_metadata.drop(['vote_average','vote_count'], axis = 1, inplace = True)\n",
        "movies_metadata.drop(['production_companies'], axis = 1, inplace = True)\n",
        "\n",
        "#change belongs_to_collection null value to no collection value\n",
        "movies_metadata['belongs_to_collection'].replace(np.nan,'no collection', inplace = True)\n",
        "\n",
        "#remove null value\n",
        "movies_metadata.dropna(inplace = True)\n",
        "\n",
        "#drop when 'genres', 'proudction_countries' has null value\n",
        "movies_metadata = movies_metadata.loc[movies_metadata['genres'] != \"[]\"]\n",
        "movies_metadata = movies_metadata.loc[movies_metadata['production_countries'] != \"[]\"]\n",
        "\n",
        "#if 'id' is in date format -> invalid\n",
        "movies_metadata['isIdRight'] = movies_metadata['id'].str.contains('|'.join('-'))\n",
        "movies_metadata = movies_metadata[movies_metadata['isIdRight'] == False]\n",
        "movies_metadata.drop(['isIdRight'], axis = 1, inplace = True)\n",
        "\n",
        "#replace missing values with null values\n",
        "movies_metadata['overview'] = movies_metadata['overview'].fillna('')\n",
        "\n",
        "#change the values of index id to numeric form for future merge\n",
        "movies_metadata.loc['id'] = movies_metadata['id'].astype('str')\n",
        "movies_metadata['id'] = pd.to_numeric(movies_metadata['id'])\n",
        "movies_metadata_title = movies_metadata[[\"imdb_id\", \"title\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83875fe8",
      "metadata": {
        "id": "83875fe8"
      },
      "source": [
        "Credit preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9947739d",
      "metadata": {
        "id": "9947739d"
      },
      "outputs": [],
      "source": [
        "#remove null value\n",
        "credits = credits.loc[credits['cast'] != \"[]\"]\n",
        "credits = credits.loc[credits['crew'] != \"[]\"]\n",
        "#change the values of index id to numeric form for future merge\n",
        "credits['id'] = pd.to_numeric(credits['id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d60850c",
      "metadata": {
        "id": "6d60850c"
      },
      "source": [
        "Keyword preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9e68437",
      "metadata": {
        "id": "f9e68437"
      },
      "outputs": [],
      "source": [
        "keywords['id'] = keywords['id'].astype('int')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df8d7666",
      "metadata": {
        "id": "df8d7666"
      },
      "source": [
        "### Test dataset preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5fe84d6",
      "metadata": {
        "id": "f5fe84d6"
      },
      "outputs": [],
      "source": [
        "test['rating'] = pd.to_numeric(test['rating'])\n",
        "\n",
        "test = pd.merge(test, movies_metadata, on=\"imdb_id\", how=\"left\")\n",
        "test = test.dropna()\n",
        "test = test[['userId','imdb_id','rating']]\n",
        "\n",
        "#we will use just 1000 data for fast test\n",
        "test = test.iloc[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b44b0088",
      "metadata": {
        "id": "b44b0088"
      },
      "source": [
        "## Recommendation system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfc9db56",
      "metadata": {
        "id": "bfc9db56"
      },
      "outputs": [],
      "source": [
        "#Movie recommendation system function\n",
        "#Input : ratings, movie_metadata, credits, keywords, test\n",
        "#Output : matrix factorization, content-based recommendation's test dt's RMSE and user input's result\n",
        "def recommendataion_system(ratings, movies_metadata, credits, keywords, test):\n",
        "    #Collaborative Filtering,matrix factorization recommendation\n",
        "    all_movie_df = pd.merge(movies_metadata, credits)\n",
        "    all_movie_df = pd.merge(all_movie_df, ratings)\n",
        "    \n",
        "    user_id = float(input('order of user : '))\n",
        "    result = mf_giantfunction(user_id,test,all_movie_df)\n",
        "    print('Collaborative matrix factorization recommendation result--------------------')\n",
        "    result = result[['title', 'imdb_id']]\n",
        "    print(result)\n",
        "    \n",
        "    #content-based recommendation\n",
        "    RMSE, accuracy = content_based_recommendation_measure(keywords, credits, movies_metadata, test)\n",
        "    print('content-based recommendation result-------------------')\n",
        "    print('RMSE : ',round(RMSE, 2),'accuracy',round(accuracy, 2))\n",
        "    recommend_movie_list = content_based_recommendation_user(keywords, credits, movies_metadata)\n",
        "    print(recommend_movie_list)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71112e06",
      "metadata": {
        "id": "71112e06"
      },
      "source": [
        "Functions for Collaborative Filtering recommendation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "819dd0c6",
      "metadata": {
        "id": "819dd0c6"
      },
      "outputs": [],
      "source": [
        "#Function that combines the processes of matrix factorization and returns recommended movies by performing matrix factorization from matrix creation\n",
        "#Input : userid(int) / test_data(string)\n",
        "#Output : recomm_movies(Dataframe)\n",
        "def mf_giantfunction(userid,test_data,all_movie_df):\n",
        "    #Function that create a matrix from dataset\n",
        "    #Input : data_name(string)\n",
        "    #Output : user_data(Dataframe)\n",
        "    def data_preprocessing_matrix(data_name):\n",
        "        data = data_name.drop_duplicates() #Drop the duplicate data\n",
        "        data = data.reset_index(drop=True) #Reset the index due to drop data\n",
        "        data = data.dropna()#Drop the nan data\n",
        "        user_df = data.pivot(index='userId', columns='imdb_id', values='rating').fillna(0) #Create matrix between userid and imdbid\n",
        "        user_data = pd.DataFrame(user_df)\n",
        "        return user_data\n",
        "    #Function that calculate RMSE Score to evaluate the Predict matrix\n",
        "    #Input : A(Full Matrix) / U & V(Partial Matrix) / Non_zeros(List)\n",
        "    #Output : rmse(Int)\n",
        "    def get_rmse(A, U, V, non_zeros):\n",
        "        #Make full predict matrix use P and Q.T\n",
        "        full_pred_matrix = np.dot(U, V.T)#Convert v matrix to transposition matrix for full matrix\n",
        "        #Save users who evaluated movies stored in y_non_zero\n",
        "        x_non_zero = [non_zero[0] for non_zero in non_zeros]\n",
        "        #Store movies that have already been rated by users stored in x_non_zero\n",
        "        y_non_zero = [non_zero[1] for non_zero in non_zeros]\n",
        "        #Save Real Rating score\n",
        "        A_non_zeros = A[x_non_zero, y_non_zero]\n",
        "        #Make predictive metrics using information from x_non_zero and y_non_zero\n",
        "        full_pred_matrix_non_zeros = full_pred_matrix[x_non_zero, y_non_zero]\n",
        "        #Calculate mse score and rmse score\n",
        "        mse = mean_squared_error(A_non_zeros, full_pred_matrix_non_zeros)\n",
        "        rmse = np.sqrt(mse)\n",
        "        return rmse\n",
        "    #Function that perform matrix factorization used SGD\n",
        "    #Input : A(Dataframe)\n",
        "    #Output : U & V(Numpy Array)\n",
        "    #Ref: https://big-dream-world.tistory.com/69\n",
        "    def matrix_factorization(A): \n",
        "        R = A.values\n",
        "        #Get the actual size of the training dataset\n",
        "        num_users, num_movies = R.shape\n",
        "\n",
        "        #Hyperparameter of Matrix factorization and SGD\n",
        "        K=100#Finish to update\n",
        "        steps = 400 #Finish to update\n",
        "        learning_rate=0.001 #Finish to update\n",
        "        r_lambda = 0.01\n",
        "    \n",
        "        np.random.seed(42)\n",
        "        u = np.random.normal(scale=1./K, size=(num_users, K)) #Create virtual random matrix with the size of user and the value of k \n",
        "        v = np.random.normal(scale=1./K, size=(num_movies, K)) #Create virtual random matrix with the size of movie and the value of k \n",
        "        \n",
        "        #Save the part of the training dataset that has already been evaluated to a list\n",
        "        non_zeros = [ (i, j, R[i,j]) for i in range(num_users) for j in range(num_movies) if R[i,j] > 0 ]\n",
        "  \n",
        "        #Update P & Q matrix used SGD\n",
        "        for step in range(steps):\n",
        "            for i, j, r in non_zeros:\n",
        "                # Calculate error\n",
        "                    err = r - np.dot(u[i, :], v[j, :].T)\n",
        "                # Update by applying SGD\n",
        "                    u[i,:] = u[i,:] + learning_rate*(err * v[j, :] - r_lambda*u[i,:])\n",
        "                    v[j,:] = v[j,:] + learning_rate*(err * u[i, :] - r_lambda*v[j,:])\n",
        "        #Calculate RMSE\n",
        "            rmse = get_rmse(R, u, v, non_zeros)\n",
        "            if(step+1==steps):\n",
        "                print(\"### Final step is finish, The rmse Score : \", round(rmse,2))\n",
        "        return u, v\n",
        "    \n",
        "    #Function that organizes movies that users haven't seen\n",
        "    #Input: ratings_matrix(Dataframe) / id(Int)\n",
        "    #Output: unseen_movie(List)\n",
        "    def get_unseen_list(ratings_matrix, id):\n",
        "        user_rating = ratings_matrix.loc[id,:] #Extract only the parts that match the user ID\n",
        "        seen_movie = user_rating[ user_rating > 0].index.tolist() #The part with a value greater than 0 is already a movie, so only that part is saved\n",
        "        movies_list = ratings_matrix.columns.tolist() #Convert movie titles saved by column names to a list\n",
        "        unseen_movie = [ movie for movie in movies_list if movie not in seen_movie] #Save movie titles that are not included in this movie list\n",
        "        return unseen_movie\n",
        "    \n",
        "    #train model\n",
        "    data = all_movie_df[['userId','title','imdb_id','rating']]\n",
        "    traindf = data_preprocessing_matrix(data) #Make matrix\n",
        "    u,v = matrix_factorization(traindf) #Matrix factorization\n",
        "    pred_matrix = np.dot(u,v.T)#Convert v matrix to transposition matrix for full matrix\n",
        "    ratings_pred_matrix = pd.DataFrame(data=pred_matrix, index= traindf.index,columns = traindf.columns) \n",
        "    #test model\n",
        "    testdf = data_preprocessing_matrix(test_data)#Make matrix\n",
        "    u_tt,v_tt = matrix_factorization(testdf) #Matrix factorization\n",
        "    pred_matrix_tt = np.dot(u_tt,v_tt.T)#Convert v matrix to transposition matrix for full matrix\n",
        "    ratings_pred_matrix_tt = pd.DataFrame(data=pred_matrix_tt, index= testdf.index,columns = testdf.columns)\n",
        "    #Calculate RMSE SCORE to Evaluate the model\n",
        "    mse = mean_squared_error(testdf, ratings_pred_matrix_tt) #Test dataset and Matrix made from test dataset\n",
        "    rmse = np.sqrt(mse) \n",
        "    print(\"The RMSE Score evaluated using test dataset is : \" + str(rmse))\n",
        "    #Recommendation \n",
        "    unseen_list = get_unseen_list(traindf, userid)\n",
        "    recomm_movies = ratings_pred_matrix.loc[userid, unseen_list].sort_values(ascending=False)[:5] #Extract only the top five movies\n",
        "    recomm_movies = pd.DataFrame(data=recomm_movies.values,index=recomm_movies.index,columns=['pred_score'])\n",
        "    recomm_movies = pd.merge(recomm_movies, movies_metadata_title, on = 'imdb_id', how = 'left') #To show movie title\n",
        "    \n",
        "    return recomm_movies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd01d71d",
      "metadata": {
        "id": "dd01d71d"
      },
      "source": [
        "Functions for content-based recommendation system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e9e85a3",
      "metadata": {
        "id": "9e9e85a3"
      },
      "outputs": [],
      "source": [
        "#Function that selects the top 6 rating for each user and calculates and returns the RMSE and acuity of the entire user\n",
        "#Input : keywords, credits, movies_metadata, test_dt\n",
        "#Output : RMSE, accuracy score's mean value\n",
        "def content_based_recommendation_measure(keywords, credits, movies_metadata, test_dt):\n",
        "    #make test_dataset_different for test\n",
        "    temp = pd.DataFrame(test_dt['userId'].value_counts())\n",
        "    row_list = list(temp.index.values)\n",
        "    temp.reset_index(inplace = True)\n",
        "    temp.columns = ['userId','rating_cnt']\n",
        "    temp = temp.drop(temp[temp['rating_cnt'] < 6].index)\n",
        "    temp2 = pd.merge(test_dt, temp)\n",
        "    temp2 = temp2[['userId','imdb_id','rating']]\n",
        "    groups = temp2.groupby('userId')\n",
        "    result = dict(list(groups))\n",
        "\n",
        "    result_top5 = {} #store test set result\n",
        "    for key, value in result.items():\n",
        "        value['rating'] = value['rating'].astype(int)\n",
        "        value.sort_values(by = 'rating', ascending = False, inplace = True)\n",
        "        result_top5[key] = value.head(6)#5+1\n",
        "    \n",
        "    #content based dataset\n",
        "    cb_trained_dt = movies_metadata\n",
        "    cb_trained_dt = cb_trained_dt.merge(credits, on='id')\n",
        "    cb_trained_dt = cb_trained_dt.merge(keywords, on='id')\n",
        "    \n",
        "    from ast import literal_eval\n",
        "    #Parse the stringified features into their corresponding python objects\n",
        "    features = ['cast', 'crew', 'keywords', 'genres']\n",
        "    for feature in features:\n",
        "        cb_trained_dt[feature] = cb_trained_dt[feature].apply(literal_eval)\n",
        "        \n",
        "    cb_trained_dt['director'] = cb_trained_dt['crew'].apply(get_director)\n",
        "\n",
        "    features = ['cast', 'keywords', 'genres']\n",
        "    for feature in features:\n",
        "        cb_trained_dt[feature] = cb_trained_dt[feature].apply(get_list)\n",
        "\n",
        "    features = ['cast', 'keywords', 'director', 'genres']\n",
        "    for feature in features:\n",
        "        cb_trained_dt[feature] = cb_trained_dt[feature].apply(clean_data)\n",
        "        \n",
        "    cb_trained_dt['soup'] = cb_trained_dt.apply(create_soup, axis=1)\n",
        "    \n",
        "    #Measure the rmse and acuity using the top 5 rating for each user\n",
        "    accuracy_list = []#list for measuring accuracy of each test cases\n",
        "    RMSE_list = []\n",
        "    for key,value in result_top5.items():\n",
        "        input_imdb_id = value.iloc[0]['imdb_id']#imdb_id of first(most liked)\n",
        "        searchTerms = cb_trained_dt[cb_trained_dt['imdb_id'] == input_imdb_id]['soup']\n",
        "        real_result = list(value['imdb_id'][1:6])\n",
        "        real_result_rating = list(value['rating'][1:6])\n",
        "        temp_RMSE, temp_accuracy = make_recommendation_for_accuracy(cb_trained_dt, searchTerms, real_result, real_result_rating)\n",
        "        accuracy_list.append(temp_accuracy)\n",
        "        RMSE_list.append(temp_RMSE)\n",
        "    return sum(RMSE_list) / len(RMSE_list), sum(accuracy_list) / len(accuracy_list)\n",
        "#Function that measures and outputs RMSE, acuity of test data in content-based\n",
        "#Input : keywords, credits, movies_metadata, test_dt\n",
        "#Output : RMSE, accuracy\n",
        "def make_recommendation_for_accuracy(cb_trained_dt, searchTerms, real_result, rating):\n",
        "    \n",
        "    metadata = cb_trained_dt\n",
        "    new_row = metadata.iloc[-1,:].copy() #creating a copy of the last row of the \n",
        "  #dataset, which we will use to input the user's input\n",
        "\n",
        "  #grabbing the new wordsoup from the user\n",
        "    new_row.iloc[-1] = \" \".join(searchTerms) #adding the input to our new row\n",
        "  \n",
        "  #adding the new row to the dataset\n",
        "    #metadata = metadata.append(new_row) #-> replace because of FutureWarning\n",
        "    metadata.loc[len(metadata)] = new_row\n",
        "  \n",
        "  #Vectorizing the entire matrix as described above!\n",
        "    count = CountVectorizer(stop_words='english')\n",
        "    count_matrix = count.fit_transform(metadata['soup'])\n",
        "\n",
        "  #running pairwise cosine similarity \n",
        "    cosine_sim2 = cosine_similarity(count_matrix, count_matrix) #getting a similarity matrix\n",
        "  \n",
        "  #sorting cosine similarities by highest to lowest\n",
        "    sim_scores = list(enumerate(cosine_sim2[-1,:]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "  #matching the similarities to the movie titles and ids\n",
        "    predict_val = []\n",
        "    predict_sim = []\n",
        "    #doing just TOP 5\n",
        "    for i in range(1, 5 + 1):\n",
        "        indx = sim_scores[i][0]\n",
        "        predict_val.append(metadata['imdb_id'].iloc[indx]) #대신 사이트 주소 출력\n",
        "        predict_sim.append(sim_scores[i][1])\n",
        "    new_rating = []\n",
        "    for val in rating:\n",
        "        #new_value = ( (old_value - old_min) / (old_max - old_min) ) * (new_max - new_min) + new_min\n",
        "        new_rating_val = ( (val - 1) / (10 - 1) ) * (1 - (-1)) - 1\n",
        "        new_rating.append(new_rating_val)\n",
        "    RMSE = mean_squared_error(new_rating, predict_sim)**0.5\n",
        "    intersection_list = list(set(predict_val) & set(real_result))\n",
        "    accuracy = len(intersection_list) / 5\n",
        "    return RMSE, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "403ecd73",
      "metadata": {
        "id": "403ecd73"
      },
      "outputs": [],
      "source": [
        "#Function that receives information about genres, actors, directors, and keywords from users and returns 5 recommended movies\n",
        "#input : kewords, credits, movies_metadata\n",
        "#output : dataframe with recommendation movie_name with imdb site link\n",
        "def content_based_recommendation_user(keywords, credits, movies_metadata):\n",
        "    #content based dataset\n",
        "    cb_trained_dt = movies_metadata\n",
        "    cb_trained_dt = cb_trained_dt.merge(credits, on='id')\n",
        "    cb_trained_dt = cb_trained_dt.merge(keywords, on='id')\n",
        "    \n",
        "    from ast import literal_eval\n",
        "    #Parse the stringified features into their corresponding python objects\n",
        "    features = ['cast', 'crew', 'keywords', 'genres']\n",
        "    for feature in features:\n",
        "        cb_trained_dt[feature] = cb_trained_dt[feature].apply(literal_eval)\n",
        "        \n",
        "    #apply func : apply functions in parenthese throughout the dataframe\n",
        "    #related link : https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=wideeyed&logNo=221559041280\n",
        "    cb_trained_dt['director'] = cb_trained_dt['crew'].apply(get_director)\n",
        "\n",
        "    features = ['cast', 'keywords', 'genres']\n",
        "    for feature in features:\n",
        "        cb_trained_dt[feature] = cb_trained_dt[feature].apply(get_list)\n",
        "\n",
        "    features = ['cast', 'keywords', 'director', 'genres']\n",
        "    for feature in features:\n",
        "        cb_trained_dt[feature] = cb_trained_dt[feature].apply(clean_data)\n",
        "        \n",
        "    #In a column called soup, keyword, cast, director, and genres are all tied together and put values that can measure cosine similarity\n",
        "    cb_trained_dt['soup'] = cb_trained_dt.apply(create_soup, axis=1)\n",
        "    \n",
        "    recommend_movie_list = make_recommendation_user_input(cb_trained_dt)\n",
        "    \n",
        "    result = pd.DataFrame(recommend_movie_list, columns = ['movie_name', 'link'])\n",
        "    for idx in range(0, len(result)):\n",
        "        temp_link = result['link'][idx].replace(result['link'][idx], \"<a href = \\\"\" + result['link'][idx] + \"\\\" >\" + result['link'][idx] + \"</a>\")\n",
        "    return result\n",
        "#Function that only actors (parsing) among movie-related people are kept separately\n",
        "#input : corpus of movie_related people name\n",
        "#output : movie_director's name\n",
        "def get_director(x):\n",
        "    for i in x:\n",
        "        if i['job'] == 'Director':\n",
        "            return i['name']\n",
        "    return np.nan\n",
        "#function : get_list\n",
        "#If it is complicated with #id, etc., it is a function that parses it and returns it to the list\n",
        "def get_list(x):\n",
        "    if isinstance(x, list):\n",
        "        names = [i['name'] for i in x]\n",
        "        return names\n",
        "\n",
        "    #Return empty list in case of missing/malformed data\n",
        "    return []\n",
        "#fnction : clean_data\n",
        "#Blankout function\n",
        "def clean_data(x):\n",
        "    #Remove blanks inside the list -> [cast, keywords, genres]\n",
        "    if isinstance(x, list):\n",
        "        return [str.lower(i.replace(\" \", \"\")) for i in x] #uppercase -> change lowercase + remove blanks\n",
        "    else:\n",
        "        #Remove blanks inside string -> [director]\n",
        "        if isinstance(x, str):\n",
        "            return str.lower(x.replace(\" \", \"\"))\n",
        "        else:\n",
        "            return ''\n",
        "#function : create_soup\n",
        "#Creating a corpus for cosine severity function\n",
        "def create_soup(x):\n",
        "    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres']) +' '+ ' '.join(x['overview']) + ' ' + ' '.join(x['original_title'])\n",
        "#get input and return output\n",
        "def get_searchTerms():\n",
        "    searchTerms = [] \n",
        "    \n",
        "    print('Write down about movie(Genre, actors, directors, keywords)')\n",
        "    user_input = input(\"(if multiple, please separate them with a comma)[Type 'null' if you don't want to type]\")\n",
        "    user_input = \" \".join([\"\".join(n.split()) for n in user_input.lower().split(',')])\n",
        "    if user_input != 'null':\n",
        "        searchTerms.append(user_input)\n",
        "\n",
        "    return searchTerms\n",
        "#function : make_recommendation_user_input\n",
        "#receives information about genres, actors, directors, and keywords from users and returns 5 recommended movies\n",
        "#input : cb_trained_dt\n",
        "#output : dataframe with recommendation movie_name with imdb site link\n",
        "def make_recommendation_user_input(cb_trained_dt):\n",
        "    metadata = cb_trained_dt\n",
        "    new_row = metadata.iloc[-1,:].copy() #creating a copy of the last row of the \n",
        "  #dataset, which we will use to input the user's input\n",
        "  #grabbing the new wordsoup from the user\n",
        "    searchTerms = get_searchTerms()  \n",
        "    new_row.iloc[-1] = \" \".join(searchTerms) #adding the input to our new row\n",
        "  \n",
        "  #adding the new row to the dataset\n",
        "    #metadata = metadata.append(new_row)\n",
        "    metadata.loc[len(metadata)] = new_row\n",
        "    \n",
        "  #Vectorizing the entire matrix as described above!\n",
        "    count = CountVectorizer(stop_words='english')\n",
        "    count_matrix = count.fit_transform(metadata['soup'])\n",
        "    \n",
        "  #running pairwise cosine similarity \n",
        "    cosine_sim2 = cosine_similarity(count_matrix, count_matrix) #getting a similarity matrix\n",
        "  \n",
        "  #sorting cosine similarities by highest to lowest\n",
        "    sim_scores = list(enumerate(cosine_sim2[-1,:]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "  #matching the similarities to the movie titles and ids\n",
        "    ranked_titles = []\n",
        "    for i in range(1, 5 + 1):\n",
        "        indx = sim_scores[i][0] \n",
        "        ranked_titles.append([metadata['title'].iloc[indx], 'https://imdb.com/title/' + metadata['imdb_id'].iloc[indx]]) #대신 사이트 주소 출력\n",
        "  \n",
        "    return ranked_titles"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8abe47be",
      "metadata": {
        "id": "8abe47be"
      },
      "source": [
        "Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22cbb0df",
      "metadata": {
        "id": "22cbb0df"
      },
      "outputs": [],
      "source": [
        "recommendataion_system(ratings, movies_metadata, credits, keywords, test)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}